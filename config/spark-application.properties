# GCS to Databricks Spark Pipeline Configuration

# Google Cloud Configuration
gcp.project.id=your-gcp-project-id
gcp.input.file=gs://your-gcp-project-id/data/products.csv
gcp.temp.location=gs://your-gcp-project-id/temp
gcp.staging.location=gs://your-gcp-project-id/staging

# BigQuery Configuration (for intermediate storage)
bigquery.dataset=temp_dataset
bigquery.table=temp_products

# Databricks Configuration
databricks.host=your-databricks-host.cloud.databricks.com
databricks.http.path=/sql/1.0/warehouses/your-warehouse-id
databricks.token=your-databricks-access-token
databricks.database=default
databricks.table=products

# Pipeline Configuration
pipeline.job.name=gcs-to-databricks-spark-pipeline
pipeline.parallelism=4

# Spark Configuration
spark.master=local[*]
spark.driver.memory=2g
spark.executor.memory=2g
spark.executor.cores=2
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.sql.execution.arrow.pyspark.enabled=true

# Logging Configuration
logging.level=INFO
logging.pattern=%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n

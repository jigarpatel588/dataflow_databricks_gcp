# GCS to Databricks Spark Pipeline Configuration

# Google Cloud Configuration
gcp.project.id=YOUR_PROJECT_ID
gcp.input.file=gs://YOUR_BUCKET/csv/products.csv
gcp.temp.location=gs://YOUR_BUCKET/temp
gcp.staging.location=gs://YOUR_BUCKET/staging

# BigQuery Configuration (for intermediate storage)
bigquery.dataset=temp_dataset
bigquery.table=temp_products

# Databricks Configuration
databricks.host=YOUR_DATABRICKS_HOST
databricks.http.path=/sql/1.0/warehouses/YOUR_WAREHOUSE_ID
databricks.token=YOUR_DATABRICKS_TOKEN
databricks.database=default
databricks.table=products

# Pipeline Configuration
pipeline.job.name=gcs-to-databricks-spark-pipeline
pipeline.parallelism=4

# Spark Configuration
spark.master=local[*]
spark.driver.memory=2g
spark.executor.memory=2g
spark.executor.cores=2
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.sql.execution.arrow.pyspark.enabled=true

# Logging Configuration
logging.level=INFO
logging.pattern=%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n